\section{Cenário de uso}
    \label{sec:usage_scenario}

    O cenário de uso da nossa implementação consiste em:
    \begin{itemize}
        \item dada a localização do fiscal, indicar o hospital mais próximo;
        \item dado o hospital, indicar os $x$ pacientes mais similares ao informado.
    \end{itemize}
    Para isso, precisamos criar um \textit{dataset} com essas informações.

    Toda a preparação do cenário de uso, incluindo a geração do \textit{dataset}, podem ser visualizadas no \textit{Jupyter Notebook} em \url{https://nbviewer.jupyter.org/github/lucasresck/data-structures-algorithms/blob/main/notebooks/k_d_tree.ipynb}. O \textit{dataset}, por sua vez, pode ser consultado em \url{https://github.com/lucasresck/data-structures-algorithms/blob/main/data/hospitals.json}.

    \subsection{\textit{Dataset}}

        Nosso \textit{dataset} contém 10 hospitais, com 7 pacientes cada. Por facilidade, cada hospital possui uma localização aleatória dentro de um quadrado unitário, e cada paciente possui características aleatórias dentro dos limites informados no enunciado.
        Veja, por exemplo, o início do arquivo JSON gerado:
        \begin{alltt}
\{
    "hospitais": [
        \{
            "localização": [
                0.62,
                0.61
            ],
            "pacientes": [
                \{
                    "peso": 91,
                    "idade": 29,
                    "saturação": 90
                \},
                \{
                    "peso": 111,
                    "idade": 75,
                    "saturação": 100
                \}, ...
        \end{alltt}
        Cada hospital é um objeto que contém sua localização e uma lista de seus pacientes; o \textit{dataset}, portanto, é uma lista desses hospitais.

    \subsection{Hospital mais próximo}
        \label{sec:nearest_hospital}

        Suponhamos que o fiscal esteja na posição $(0.5, 0.5)$ do quadrado unitário. A \autoref{fig:points} mostra a localização dos hospitais e do fiscal.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.6\textwidth]{points.pdf}
            \caption{Localização dos hospitais (em azul) e do fiscal (em laranja).}
            \label{fig:points}
        \end{figure}
        Depois de montarmos a árvore k-d com dimensão $k=2$ utilizando a localização dos hospitais como nó, realizamos uma busca k-NN com $k$ igual a 1 para encontrarmos o hospital mais próximo. A saída do algoritmo é
        \begin{alltt}
[\{'node': [0.56, 0.53],
  'metadata': \{'localização': [0.56, 0.53],
   'pacientes': [\{'peso': 98, 'idade': 50, 'saturação': 98\},
    \{'peso': 129, 'idade': 81, 'saturação': 98\},
    \{'peso': 59, 'idade': 85, 'saturação': 99\},
    \{'peso': 72, 'idade': 54, 'saturação': 86\},
    \{'peso': 121, 'idade': 15, 'saturação': 90\},
    \{'peso': 96, 'idade': 64, 'saturação': 89\},
    \{'peso': 46, 'idade': 30, 'saturação': 95\}]\}\}]
        \end{alltt}
        Encontramos o hospital localizado no ponto $(0.56, 0.53)$, realmente o ponto mais próximo na \autoref{fig:points}. Além disso, pelo fato de que nossa implementação permite o armazenamento de metadados, temos também todas as informações desse hospital, incluindo a lista de pacientes e suas características.

    \subsection{Pacientes mais parecidos}

        Suponhamos que as características dos pacientes do hospital mais próximo (\autoref{sec:nearest_hospital}) são adicionadas em uma nova k-d tree, de dimensão $k=3$, sem nenhum tratamento. Quando fôssemos buscar os vizinhos mais próximos utilizando a distância euclidiana, teríamos, estaríamos priorizando algumas características em relação em outras, neste caso aquelas características com maior valor absoluto. Por exemplo, uma pessoa com saturação 80\% é completamente diferente de uma com saturação 100\%, porém, em valor absoluto, isso equivale à diferença de peso entre uma pessoa de 80 kg e 100 kg, que são parecidas neste contexto. Para evitar priorizar características, antes de inserirmos os pacientes na árvore normalizamos as \textit{features} para valores entre 0 e 1, utilizando a seguinte função:
        \begin{lstlisting}[language=Python]
def normalize(weight, age, saturation):
    return (weight-40)/90, (age-15)/75, (saturation-80)/20
        \end{lstlisting}

        Tendo a árvore k-d montada utilizando as características normalizadas, podemos consultar os pacientes mais similares. Por exemplo, suponha que o fiscal esteja buscando os três pacientes mais similares àquele com as seguintes características: peso 46 kg, idade 30 e saturação 96\%. Utilizando uma busca k-NN, com $x = 3$, temos a seguinte saída:
        \begin{alltt}
[\{'node': [0.06666666666666667, 0.2, 0.75],
  'metadata': \{'peso': 46, 'idade': 30, 'saturação': 95\}\},
 \{'node': [0.6444444444444445, 0.4666666666666667, 0.9],
  'metadata': \{'peso': 98, 'idade': 50, 'saturação': 98\}\},
 \{'node': [0.35555555555555557, 0.52, 0.3],
  'metadata': \{'peso': 72, 'idade': 54, 'saturação': 86\}\}]
        \end{alltt}
        Isto é, o paciente mais parecido tem as mesmas características, com exceção da saturação, que variou em 1\% (46 kg, 30 anos e 95\%). Nessa saída do algoritmo, podemos observar que as chaves nos nós foram armazenadas no formato normalizado.
